#!/usr/bin/env python2.7

import json
import os
import os.path
import re
import subprocess


DOCUMENTATION = '''
---
module: repair_docker_data_volumes
short_description: fixes issues with docker data volumes that occur during upgrades.
description:
  - This module fixes issues with data volumes that might occur when updating Docker.
  - It will stop the docker daemon once it decides what to do.
  - It handles the following issues:
  - 1) when updating to docker 1.9.x, if there is a local volume with its data in
    '/var/lib/docker/vfs/dir/<id>', Docker will create an entry in '/var/lib/docker/volumes'
    for that volume, with a config file in '/var/lib/docker/volumes/<id>/config.json'
    pointing to data at '/var/lib/docker/volumes/<id>/_data', and it will add a symlink at
    the '_data' path pointing to the old data directory, so that the volume stays functional.
    However, if you update from a pre-1.9 version to 1.10 or greater, the config file is still
    created, it still points to the '_data' path, but the symlink is not created and the volume
    is nonfunctional.
  - The module fixes the issue by creating the symlink.
  - 2) Before 1.10, the volumes were represented in container config in a "Volumes" section,
    and after 1.10 they are represented by a "Mounts" section. Upgrading Docker doesn't
    translate from one to the other, so any container that has an unmapped volume mounted
    before the upgrade will be broken after the upgrade.
  - Issue 2 is officialy documented at https://github.com/docker/docker/issues/20079
  - The module fixes the issue by updating the config file in
    '/var/lib/docker/containers/<id>/config.v2.json'.
  - There are two cases handled. The primary case is where the container owned the
    volumes. In that case, after the upgrade the "Volumes" and "VolumesRW" keys are still
    present in the upgraded data file, and that information can be used directly.
  - The secondary case is where the container did not own the volumes, but was pulling them
    in from another container using "--volumes-from". In this case, the "Volumes" and
    "VolumesRW" keys appear to be removed during the update process, as well as the
    "AppliedVolumesFrom" key. In order to address this case, we rely on configuration data
    collected before the package update (by the collect_container_configs module).
requirements:
  - This module should be run after the docker package upgrade.
  - If changes happen, the docker daemon should be started again.
  - i.e. run this with "notify: restart docker"
options:
  fix_vfs_links:
    description:
      - should we fix the VFS links (issue 1)?
    required: true
  fix_container_mountpoints:
    description:
      - should we fix the container mountpoints (issue 2)?
    required: true
  old_container_configs:
    description:
      - config data from containers, before the package update
    required: true
'''

CONTAINER_ROOT_DIR = '/var/lib/docker/containers'
CONFIG_V2_FILENAME = 'config.v2.json'
CONFIG_V3_FILENAME = 'config.v3.json'
BACKUP_FILENAME = 'config.v2.json.pre_volume_fix'
VFS_BASE_DIR = '/var/lib/docker/vfs/dir'
VOLUMES_ROOT_DIR = '/var/lib/docker/volumes'
LOCAL_DRIVER = 'local'

VFS_MOUNT_REGEX = re.compile(VFS_BASE_DIR + "/([a-f0-9]+)$")
VOLUME_MOUNT_REGEX = re.compile(VOLUMES_ROOT_DIR + "/([a-f0-9]+)/_data")


class Volume(object):
    def __init__(self, driver, name):
        self.driver = driver
        self.name = name

    def inspect(self, module):
        try:
            inspect_output = subprocess.check_output(
                ['docker', 'volume', 'inspect', self.name],
                stderr=subprocess.STDOUT)
            return json.loads(inspect_output)[0]
        except subprocess.CalledProcessError, c:
            module.fail_json(
                msg="failure while inspecting volume {}".format(self.name),
                exception=str(c))

    @property
    def is_local(self):
        return self.driver == LOCAL_DRIVER

    @property
    def vfs_path(self):
        return os.path.join(VFS_BASE_DIR, self.name)


def _volumes(module):
    try:
        volumes_output = subprocess.check_output(
            ['docker', 'volume', 'ls'],
            stderr=subprocess.STDOUT)

        return [
            Volume(driver=line.split()[0], name=line.split()[1])
            for line in volumes_output.split('\n')[1:] if line]

    except subprocess.CalledProcessError, c:
        module.fail_json(
            msg="failure while listing volumes",
            exception=str(c))


class CreateSymlinkOperation(object):
    def __init__(self, source_path, link_path):
        self.source_path = source_path
        self.link_path = link_path

    def run(self, module):
        os.symlink(self.source_path, self.link_path)

    def desc(self):
        return "creating symlink from {} to {}".format(self.source_path, self.link_path)


def fix_volume_mount_missing_symlinks(module):
    for volume in _volumes(module):
        if not volume.is_local:
            # don't deal with volumes that don't use the "local" driver.
            continue

        inspect_data = volume.inspect(module)
        if os.path.exists(inspect_data['Mountpoint']):
            # mount point for this volume is non-broken
            continue

        if not os.path.exists(volume.vfs_path):
            # mount data isn't where we expect it to be
            continue

        yield CreateSymlinkOperation(volume.vfs_path, inspect_data['Mountpoint'])


class RewriteConfigOperation(object):
    def __init__(self, path, backup_path, updated_content):
        self.path = path
        self.backup_path = backup_path
        self.updated_content = updated_content

    def run(self, module):
        old_stat = os.stat(self.path)
        os.rename(self.path, self.backup_path)
        with open(self.path, 'w') as f:
            f.write(self.updated_content)
        os.chmod(self.path, old_stat.st_mode)
        os.chown(self.path, old_stat.st_uid, old_stat.st_gid)

    def desc(self):
        return "updating config file {}".format(self.path)


def fix_container_configs(module, old_container_configs, skipped_container_reasons):

    for container in os.listdir(CONTAINER_ROOT_DIR):
        container_path = os.path.join(CONTAINER_ROOT_DIR, container)
        if not os.path.isdir(container_path):
            # we only expect directories here. Ignore the unexpected
            continue

        # verify that there's a v2 config file there
        config_v2_path = os.path.join(container_path, CONFIG_V2_FILENAME)
        if not os.path.isfile(config_v2_path):
            skipped_container_reasons.append(
                'skipped {} because v2 path does not exist'.format(container))
            continue

        # Future Docker updates may continue to change this file. If the Docker team does the
        # same thing they did last time, they'll add a "config.v3.json" file and leave a
        # now-non-functional 'config.v2.json' file in place. If so, skip this file.
        config_v3_path = os.path.join(container_path, CONFIG_V3_FILENAME)
        if os.path.isfile(config_v3_path):
            skipped_container_reasons.append('skipped {} because v3 path exists'.format(container))
            continue

        # find pre-update config
        old_config = old_container_configs.get(container)

        # now read config JSON
        with file(config_v2_path, 'r') as f:
            config = json.load(f)

        # What we're looking for is images that have a config section like this:
        #
        #   "Volumes": {
        #       "/opt/some/volume": "/var/lib/docker/vfs/dir/33ff....e78b",
        #       "/opt/mapped/volume": "/opt/mapped/volume/outside"
        #    },
        #    "VolumesRW": {
        #        "/opt/some/volume": true,
        #        "/opt/mapped/volume": false
        #   }
        #
        # and don't have a "MountPoints" key.
        if 'MountPoints' in config and config['MountPoints']:
            # there is already a "MountPoints" section, so ignore this container
            skipped_container_reasons.append(
                'skipped {} because MountPoints exists'.format(container))
            continue

        if 'Volumes' in config:
            # there is a "Volumes" key in the current config, migrate that data
            volumes = config['Volumes']
            volumes_rw = config['VolumesRW']
        elif 'Volumes' in old_config:
            # there is no volume data in the current config, but it's there in the old config.
            # We see this in containers that use "--volumes-from".
            # Use the old data
            volumes = old_config['Volumes']
            volumes_rw = old_config['VolumesRW']
        else:
            # there are no volumes on this container, so there's nothing to do
            skipped_container_reasons.append(
                'skipped {} because Volumes does not exist'.format(container))
            continue

        # This config should turn into:
        #
        #   "MountPoints": {
        #       "/opt/some/volume": {
        #           "Destination": "/opt/some/volume",
        #           "Driver": "local",
        #           "Name": "33ff....e78b",
        #           "Named": false,
        #           "Propagation": "",
        #           "RW": true,
        #           "Relabel": "",
        #           "Source": ""
        #       }
        #       "/opt/mapped/volume": {
        #           "Destination": "/opt/mapped/volume",
        #           "Driver": "",
        #           "Name": "",
        #           "Named": false,
        #           "Propagation": "rprivate",
        #           "RW": false,
        #           "Relabel": "",
        #           "Source": "/opt/mapped/volume/outside"
        #       }
        #   },
        #
        mountpoints = {}
        for internal_path, external_path in volumes.iteritems():
            rw = volumes_rw[internal_path]

            match_result = (VFS_MOUNT_REGEX.match(external_path)
                            or VOLUME_MOUNT_REGEX.match(external_path))
            if match_result:
                # unmapped volume
                volume_id = match_result.group(1)
                mountpoints[internal_path] = {
                    'Destination': internal_path,
                    'Driver': 'local',
                    'Name': volume_id,
                    'Named': False,
                    'Propagation': '',
                    'RW': rw,
                    'Relabel': '',
                    'Source': ''
                }
            else:
                # mapped volume
                mountpoints[internal_path] = {
                    'Destination': internal_path,
                    'Driver': '',
                    'Name': '',
                    'Named': False,
                    'Propagation': 'rprivate',
                    'RW': rw,
                    'Relabel': '',
                    'Source': external_path
                }
        config['MountPoints'] = mountpoints

        backup_path = os.path.join(container_path, BACKUP_FILENAME)
        yield RewriteConfigOperation(config_v2_path, backup_path, json.dumps(config))


def main():
    module = AnsibleModule(
        argument_spec=dict(
            fix_vfs_links=dict(required=True, type='bool'),
            fix_container_mountpoints=dict(required=True, type='bool'),
            old_container_configs=dict(required=True)),
        supports_check_mode=True)

    # We need to figure out what to do before doing it.
    # That's because, when we execute our operations, we shut down the docker daemon first -
    # but figuring out what to do, in some cases, requires us to query the docker daemon.

    old_container_configs = module.params['old_container_configs']

    if isinstance(old_container_configs, basestring):
        with file('/var/tmp/config_after.json', 'w') as f:
            f.write(old_container_configs)
        try:
            old_container_configs = json.loads(old_container_configs)
        except ValueError, v:
            module.fail_json(
                    msg="failed parsing JSON config",
                    changed=False,
                    exception=str(v),
                    old_container_configs=old_container_configs,
                    tasks_performed=[])

    operations = []
    if module.params['fix_vfs_links']:
        operations.extend(fix_volume_mount_missing_symlinks(module))

    skipped_container_reasons = []
    if module.params['fix_container_mountpoints']:
        operations.extend(fix_container_configs(module,
                                                old_container_configs,
                                                skipped_container_reasons))

    if not operations:
        module.exit_json(changed=False, tasks_performed=[])

    if not module.check_mode:
        try:
            subprocess.check_call(['service', 'docker', 'stop'])
        except subprocess.CalledProcessError, c:
            module.fail_json(
                msg="failure while stopping docker daemon",
                exception=str(c))

    tasks_performed = []
    for operation in operations:
        if not module.check_mode:
            try:
                operation.run(module)
            except Exception, e:
                module.fail_json(
                    changed=bool(tasks_performed),
                    msg=("failed during: " + operation.desc()),
                    exception=str(e),
                    tasks_performed=tasks_performed)

        tasks_performed.append(operation.desc())

    module.exit_json(
            changed=True,
            tasks_performed=tasks_performed,
            skipped_container_reasons=skipped_container_reasons,
            old_container_configs=old_container_configs)


from ansible.module_utils.basic import *
if __name__ == '__main__':
    main()
